# Reusable Workflow: Test Suite
# Handles unit tests, integration tests, E2E tests, and coverage reporting

name: Test Suite

on:
  workflow_call:
    inputs:
      python-version:
        description: 'Python version to use'
        type: string
        default: '3.13'
      node-version:
        description: 'Node.js version to use'
        type: string
        default: '22'
      coverage-threshold:
        description: 'Minimum coverage percentage'
        type: number
        default: 75
      skip-e2e:
        description: 'Skip E2E tests'
        type: boolean
        default: false
    outputs:
      test-results:
        description: 'Overall test results summary'
        value: ${{ jobs.test-summary.outputs.results }}
      coverage-report:
        description: 'Coverage report status'
        value: ${{ jobs.coverage-merge.outputs.status }}

env:
  PYTHON_VERSION: ${{ inputs.python-version }}
  NODE_VERSION: ${{ inputs.node-version }}
  MINIMUM_COVERAGE: ${{ inputs.coverage-threshold }}

jobs:
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        test-group: [framework, api, security, plugins]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        cache-dependency-path: |
          requirements.txt
          requirements-dev.txt

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-cov pytest-asyncio pytest-mock pytest-xdist coverage
        if [ -f requirements-dev.txt ]; then pip install -r requirements-dev.txt; fi
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

    - name: Run unit tests for ${{ matrix.test-group }}
      run: |
        echo "ðŸ§ª Running unit tests for ${{ matrix.test-group }}..."
        
        case "${{ matrix.test-group }}" in
          "framework")
            # Test framework directory and existing unit tests
            if [ -d "tests/unit" ]; then
              pytest tests/unit/ -v -n auto \
                --cov=framework --cov-report=xml:coverage-framework.xml \
                --cov-report=term-missing --cov-branch \
                --timeout=300 -m "not slow and not integration" || echo "Some framework tests failed"
            fi
            if [ -d "framework/tests" ]; then
              pytest framework/tests/ -v -n auto \
                --cov=framework --cov-append \
                --timeout=300 || echo "Some framework tests failed"
            fi
            ;;
          "api")
            # Test API-related files
            if [ -d "tests/unit" ]; then
              pytest tests/unit/test_*api* tests/unit/test_fastapi* -v -n auto \
                --cov=api --cov=framework/api \
                --cov-report=xml:coverage-api.xml \
                --cov-report=term-missing --cov-branch \
                --timeout=300 -m "not slow and not integration" || echo "Some API tests failed"
            fi
            ;;
          "security")
            # Test security directory if it exists
            if [ -d "tests/security" ]; then
              pytest tests/security/ -v -n auto \
                --cov=security --cov=framework/security \
                --cov-report=xml:coverage-security.xml \
                --cov-report=term-missing --cov-branch \
                --timeout=300 -m "not slow and not integration" || echo "Some security tests failed"
            fi
            ;;
          "plugins")
            # Skip plugins if directory doesn't exist
            echo "âš ï¸ Skipping plugins tests - directory not found"
            ;;
        esac
      env:
        PORT: 8080
        WEB_UI_HOST: localhost
        ENVIRONMENT: test

    - name: Upload coverage artifact
      uses: actions/upload-artifact@v4
      with:
        name: coverage-${{ matrix.test-group }}
        path: coverage-*.xml
        retention-days: 1

  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        test-type: [api-bridge, multi-agent, session-management]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        cache-dependency-path: |
          requirements.txt
          requirements-dev.txt

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-cov pytest-asyncio pytest-mock pytest-xdist httpx
        if [ -f requirements-dev.txt ]; then pip install -r requirements-dev.txt; fi
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

    - name: Run integration tests for ${{ matrix.test-type }}
      run: |
        echo "ðŸ”— Running integration tests for ${{ matrix.test-type }}..."
        
        case "${{ matrix.test-type }}" in
          "api-bridge")
            pytest tests/integration/test_api_bridge.py tests/integration/test_fast_api_bridge.py -v -n auto \
              --cov=api --cov=framework --cov-append \
              --cov-report=xml:coverage-integration-api.xml \
              --timeout=600 -m "integration"
            ;;
          "multi-agent")
            pytest tests/integration/test_multi_agent*.py -v -n auto \
              --cov=framework --cov-append \
              --cov-report=xml:coverage-integration-agents.xml \
              --timeout=600 -m "integration"
            ;;
          "session-management")
            pytest tests/session/ tests/integration/test_*session* -v -n auto \
              --cov=framework --cov-append \
              --cov-report=xml:coverage-integration-sessions.xml \
              --timeout=600 -m "integration"
            ;;
        esac
      env:
        PORT: 8080
        WEB_UI_HOST: localhost
        ENVIRONMENT: test
        DATABASE_URL: sqlite:///test.db

    - name: Upload integration coverage
      uses: actions/upload-artifact@v4
      with:
        name: coverage-integration-${{ matrix.test-type }}
        path: coverage-integration-*.xml
        retention-days: 1

  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        cache-dependency-path: |
          requirements.txt
          requirements-dev.txt

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-benchmark pytest-asyncio
        if [ -f requirements-dev.txt ]; then pip install -r requirements-dev.txt; fi
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

    - name: Run performance tests
      run: |
        echo "âš¡ Running performance benchmarks..."
        pytest tests/performance/ framework/tests/performance/ -v \
          --benchmark-only \
          --benchmark-json=benchmark-results.json \
          --timeout=900 \
          -m "performance or benchmark"
      env:
        PORT: 8080
        WEB_UI_HOST: localhost
        ENVIRONMENT: test

    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: performance-results
        path: benchmark-results.json
        retention-days: 5

  e2e-tests:
    name: End-to-End Tests
    runs-on: ubuntu-latest
    if: ${{ !inputs.skip-e2e }}
    strategy:
      fail-fast: false
      matrix:
        shard: [1, 2]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        cache-dependency-path: |
          requirements.txt
          requirements-dev.txt

    - name: Set up Node.js ${{ env.NODE_VERSION }}
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'

    - name: Cache Playwright browsers
      uses: actions/cache@v4
      with:
        path: ~/.cache/ms-playwright
        key: playwright-browsers-${{ hashFiles('package-lock.json') }}
        restore-keys: |
          playwright-browsers-

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-asyncio playwright
        if [ -f requirements-dev.txt ]; then pip install -r requirements-dev.txt; fi
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

    - name: Install Node.js dependencies
      run: npm ci

    - name: Install Playwright browsers
      run: npx playwright install --with-deps chromium

    - name: Start application for E2E tests
      run: |
        echo "ðŸš€ Starting application for E2E tests..."
        python -m uvicorn main:app --host 0.0.0.0 --port 8080 &
        sleep 10
        # Health check
        curl -f http://localhost:8080/health || (echo "App failed to start" && exit 1)
      env:
        PORT: 8080
        WEB_UI_HOST: localhost
        ENVIRONMENT: test

    - name: Run E2E tests (Python)
      run: |
        echo "ðŸ§ª Running Python E2E tests..."
        pytest tests/e2e/ -v --timeout=900 -m "e2e"
      env:
        PORT: 8080
        WEB_UI_HOST: localhost
        BASE_URL: http://localhost:8080

    - name: Run E2E tests (JavaScript) - Shard ${{ matrix.shard }}
      run: |
        echo "ðŸ§ª Running JavaScript E2E tests (Shard ${{ matrix.shard }}/2)..."
        npx playwright test --shard=${{ matrix.shard }}/2
      env:
        BASE_URL: http://localhost:8080

    - name: Upload E2E test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: e2e-results-shard-${{ matrix.shard }}
        path: |
          playwright-report/
          test-results/
          coverage/
        retention-days: 5

  coverage-merge:
    name: Coverage Analysis
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests]
    if: always()
    outputs:
      status: ${{ steps.coverage-check.outputs.status }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install coverage tools
      run: |
        python -m pip install --upgrade pip
        pip install coverage

    - name: Download all coverage artifacts
      uses: actions/download-artifact@v4
      with:
        pattern: coverage-*
        merge-multiple: true

    - name: Merge coverage reports
      run: |
        echo "ðŸ“Š Merging coverage reports..."
        
        # Combine XML coverage files
        pip install coverage[toml]
        
        # Create a combined coverage report
        if ls coverage-*.xml 1> /dev/null 2>&1; then
          echo "Found coverage files:"
          ls -la coverage-*.xml
          
          # Use coverage.py to combine reports
          coverage combine coverage-*.xml || echo "Some coverage files may be incompatible"
          coverage report --show-missing --fail-under=${{ env.MINIMUM_COVERAGE }} || {
            echo "âŒ Coverage below threshold of ${{ env.MINIMUM_COVERAGE }}%"
            echo "status=failed" >> coverage-status.txt
            exit 1
          }
          coverage xml -o combined-coverage.xml
          echo "status=passed" >> coverage-status.txt
        else
          echo "âš ï¸ No coverage files found"
          echo "status=skipped" >> coverage-status.txt
        fi

    - name: Upload combined coverage
      uses: actions/upload-artifact@v4
      with:
        name: combined-coverage
        path: |
          combined-coverage.xml
          coverage-status.txt
        retention-days: 30

    - name: Upload to Codecov
      if: env.CODECOV_TOKEN != ''
      uses: codecov/codecov-action@v4
      with:
        file: ./combined-coverage.xml
        flags: combined
        name: codecov-combined
        fail_ci_if_error: false
        token: ${{ secrets.CODECOV_TOKEN }}

    - name: Set coverage status
      id: coverage-check
      run: |
        if [ -f coverage-status.txt ]; then
          STATUS=$(cat coverage-status.txt)
          echo "status=$STATUS" >> $GITHUB_OUTPUT
          echo "Coverage status: $STATUS"
        else
          echo "status=unknown" >> $GITHUB_OUTPUT
        fi

  test-summary:
    name: Test Results Summary
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, performance-tests, e2e-tests, coverage-merge]
    if: always()
    outputs:
      results: ${{ steps.summary.outputs.results }}
    
    steps:
    - name: Generate test summary
      id: summary
      run: |
        echo "ðŸ“Š Test Suite Results Summary"
        echo "============================"
        echo "Unit Tests: ${{ needs.unit-tests.result }}"
        echo "Integration Tests: ${{ needs.integration-tests.result }}"
        echo "Performance Tests: ${{ needs.performance-tests.result }}"
        echo "E2E Tests: ${{ needs.e2e-tests.result }}"
        echo "Coverage Analysis: ${{ needs.coverage-merge.result }}"
        echo ""
        
        # Count failures
        failures=0
        if [[ "${{ needs.unit-tests.result }}" == "failure" ]]; then
          failures=$((failures + 1))
          echo "âŒ Unit tests failed"
        fi
        if [[ "${{ needs.integration-tests.result }}" == "failure" ]]; then
          failures=$((failures + 1))
          echo "âŒ Integration tests failed"
        fi
        if [[ "${{ needs.performance-tests.result }}" == "failure" ]]; then
          failures=$((failures + 1))
          echo "âŒ Performance tests failed"
        fi
        if [[ "${{ needs.e2e-tests.result }}" == "failure" ]]; then
          failures=$((failures + 1))
          echo "âŒ E2E tests failed"
        fi
        if [[ "${{ needs.coverage-merge.result }}" == "failure" ]]; then
          failures=$((failures + 1))
          echo "âŒ Coverage analysis failed"
        fi
        
        if [ $failures -eq 0 ]; then
          echo "âœ… All tests passed successfully!"
          echo "results=success" >> $GITHUB_OUTPUT
        else
          echo "âŒ $failures test suite(s) failed"
          echo "results=failure" >> $GITHUB_OUTPUT
          exit 1
        fi
