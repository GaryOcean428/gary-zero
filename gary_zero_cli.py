#!/usr/bin/env python3
"""
CLI tool for Gary-Zero Unified Logging, Monitoring & Benchmarking Framework.

Provides command-line interface for querying logs, running benchmarks,
and generating reports.
"""

import argparse
import asyncio
import json
import sys
from datetime import datetime, timedelta

from integration_demo import GaryZeroTestExecutor

from framework.benchmarking.analysis import BenchmarkAnalysis, RegressionDetector
from framework.benchmarking.harness import BenchmarkHarness
from framework.benchmarking.reporting import BenchmarkReporter
from framework.benchmarking.tasks import StandardTasks
from framework.logging.storage import SqliteStorage
from framework.logging.unified_logger import EventType, LogLevel, get_unified_logger


async def cmd_logs(args):
    """Command to query and display logs."""
    logger = get_unified_logger()

    # Parse filters
    event_type = None
    if args.event_type:
        try:
            event_type = EventType(args.event_type)
        except ValueError:
            print(f"‚ùå Invalid event type: {args.event_type}")
            print(f"Available types: {', '.join([e.value for e in EventType])}")
            return

    level = None
    if args.level:
        try:
            level = LogLevel(args.level)
        except ValueError:
            print(f"‚ùå Invalid log level: {args.level}")
            print(f"Available levels: {', '.join([l.value for l in LogLevel])}")
            return

    # Parse time range
    start_time = None
    end_time = None
    if args.since:
        try:
            hours = float(args.since)
            start_time = (datetime.now() - timedelta(hours=hours)).timestamp()
        except ValueError:
            print(f"‚ùå Invalid time format: {args.since}")
            return

    # Query events
    events = await logger.get_events(
        event_type=event_type,
        level=level,
        user_id=args.user_id,
        agent_id=args.agent_id,
        start_time=start_time,
        end_time=end_time,
        limit=args.limit,
    )

    if not events:
        print("üì≠ No events found matching criteria")
        return

    print(f"üìã Found {len(events)} events:")
    print("-" * 80)

    for event in events:
        timestamp = datetime.fromtimestamp(event.timestamp).strftime(
            "%Y-%m-%d %H:%M:%S"
        )
        duration = f" ({event.duration_ms:.1f}ms)" if event.duration_ms else ""

        print(f"[{timestamp}] {event.level.value.upper():<8} {event.event_type.value}")
        print(f"  {event.message}{duration}")

        if event.tool_name:
            print(f"  Tool: {event.tool_name}")

        if event.error_message:
            print(f"  Error: {event.error_message}")

        if args.verbose and event.metadata:
            print(f"  Metadata: {json.dumps(event.metadata, indent=2)}")

        print()


async def cmd_stats(args):
    """Command to display statistics."""
    logger = get_unified_logger()
    stats = logger.get_statistics()

    print("üìä Unified Logging Statistics")
    print("=" * 40)
    print(f"Total Events:        {stats['total_events']}")
    print(f"Events in Buffer:    {stats['events_in_buffer']}")
    print(f"Buffer Utilization:  {stats['buffer_utilization']:.1%}")
    print()

    print("Events by Type:")
    for event_type, count in sorted(stats["events_by_type"].items()):
        print(f"  {event_type:<20} {count:>8}")
    print()

    print("Events by Level:")
    for level, count in sorted(stats["events_by_level"].items()):
        print(f"  {level:<20} {count:>8}")


async def cmd_timeline(args):
    """Command to display execution timeline."""
    logger = get_unified_logger()

    # Parse time range
    start_time = None
    if args.since:
        try:
            hours = float(args.since)
            start_time = (datetime.now() - timedelta(hours=hours)).timestamp()
        except ValueError:
            print(f"‚ùå Invalid time format: {args.since}")
            return

    timeline = await logger.get_execution_timeline(
        user_id=args.user_id, agent_id=args.agent_id, start_time=start_time
    )

    if not timeline:
        print("üì≠ No timeline events found")
        return

    print(f"üïí Execution Timeline ({len(timeline)} events):")
    print("-" * 80)

    for event in timeline:
        timestamp = datetime.fromtimestamp(event.timestamp).strftime("%H:%M:%S")
        duration = f" ({event.duration_ms:.1f}ms)" if event.duration_ms else ""

        print(f"[{timestamp}] {event.event_type.value:<20} {event.message}{duration}")


async def cmd_benchmark(args):
    """Command to run benchmarks."""
    print("üß™ Running benchmark suite...")

    # Set up harness
    harness = BenchmarkHarness(results_dir=args.output or "./benchmark_results")

    # Register tasks
    if args.tasks_file:
        try:
            task_registry = StandardTasks.load_tasks_from_file(args.tasks_file)
        except Exception as e:
            print(f"‚ùå Error loading tasks file: {e}")
            return
    else:
        task_registry = StandardTasks.get_all_standard_tasks()

    for task_id, task in task_registry.tasks.items():
        harness.register_test_case(task)

    print(f"‚úÖ Registered {len(task_registry.tasks)} tasks")

    # Register executor
    harness.register_executor("gary_zero", GaryZeroTestExecutor())

    # Register configurations with modern models
    configs = {
        "default": {"model": "gpt-4o", "temperature": 0.7},
        "optimized": {"model": "o1", "temperature": 0.3},
        "creative": {"model": "gpt-4o", "temperature": 0.8},
    }

    for config_name, config in configs.items():
        harness.register_configuration(config_name, config)

    # Run benchmarks
    task_ids = args.task_ids.split(",") if args.task_ids else None
    config_names = args.configs.split(",") if args.configs else None

    results = await harness.run_test_suite(
        task_ids=task_ids,
        config_names=config_names,
        run_parallel=not args.sequential,
        max_concurrent=args.parallel_count,
    )

    print(f"‚úÖ Completed {len(results)} benchmark runs")

    # Display summary
    successful = [r for r in results if r.status.value == "completed"]
    if successful:
        avg_score = sum(r.score for r in successful if r.score) / len(
            [r for r in successful if r.score]
        )
        print(f"üìä Success rate: {len(successful) / len(results):.1%}")
        print(f"üìä Average score: {avg_score:.3f}")

    # Generate report if requested
    if args.report:
        reporter = BenchmarkReporter(output_dir=args.output or "./benchmark_results")
        report_file = reporter.generate_summary_report(results, "CLI Benchmark Report")
        print(f"üìÑ Report saved to: {report_file}")


async def cmd_analyze(args):
    """Command to analyze benchmark results."""
    print(f"üìà Analyzing benchmark results from {args.results_dir}...")

    # Load results
    harness = BenchmarkHarness(results_dir=args.results_dir)
    results = await harness.load_results()

    if not results:
        print("‚ùå No benchmark results found")
        return

    print(f"üìä Loaded {len(results)} benchmark results")

    # Generate analysis
    analysis = BenchmarkAnalysis.calculate_summary_stats(results)

    if "error" in analysis:
        print(f"‚ùå Analysis error: {analysis['error']}")
        return

    print("\nüìà Summary Statistics:")
    print(f"  Total runs:        {analysis['total_runs']}")
    print(f"  Successful runs:   {analysis['successful_runs']}")
    print(f"  Success rate:      {analysis['success_rate']:.1%}")

    if "score_stats" in analysis:
        score_stats = analysis["score_stats"]
        print(f"  Average score:     {score_stats['mean']:.3f}")
        print(f"  Score std dev:     {score_stats['std_dev']:.3f}")
        print(
            f"  Score range:       {score_stats['min']:.3f} - {score_stats['max']:.3f}"
        )

    # Configuration comparison
    if args.compare_configs:
        comparison = BenchmarkAnalysis.compare_configurations(results)
        print("\nüîç Configuration Comparison:")

        for config_name, config_stats in comparison.items():
            if "error" not in config_stats:
                success_rate = config_stats.get("success_rate", 0)
                avg_score = config_stats.get("score_stats", {}).get("mean", 0)
                print(
                    f"  {config_name:<15} {success_rate:.1%} success, {avg_score:.3f} avg score"
                )

    # Regression detection
    if args.detect_regressions and args.baseline_period:
        try:
            hours = float(args.baseline_period)
            baseline_cutoff = (datetime.now() - timedelta(hours=hours)).timestamp()

            baseline_results = [r for r in results if r.timestamp < baseline_cutoff]
            current_results = [r for r in results if r.timestamp >= baseline_cutoff]

            if baseline_results and current_results:
                detector = RegressionDetector()
                alerts = detector.detect_regressions(baseline_results, current_results)

                if alerts:
                    print(f"\n‚ö†Ô∏è  Regression Alerts ({len(alerts)}):")
                    for alert in alerts:
                        print(
                            f"  {alert.task_id}: {alert.metric} degraded by {alert.change_percent:.1%} ({alert.severity})"
                        )
                else:
                    print("\n‚úÖ No regressions detected")
            else:
                print("\n‚ùå Insufficient data for regression analysis")

        except ValueError:
            print(f"‚ùå Invalid baseline period: {args.baseline_period}")


async def cmd_export(args):
    """Command to export data."""
    if args.type == "logs":
        # Export logs
        if args.storage == "sqlite":
            storage = SqliteStorage(args.db_path or "./logs/events.db")
            events = await storage.get_events(limit=args.limit)

            output_file = args.output or "exported_logs.json"
            with open(output_file, "w") as f:
                for event in events:
                    f.write(event.to_json() + "\n")

            print(f"‚úÖ Exported {len(events)} log events to {output_file}")
        else:
            # Export from memory
            logger = get_unified_logger()
            events = await logger.get_events(limit=args.limit)

            output_file = args.output or "exported_logs.json"
            with open(output_file, "w") as f:
                for event in events:
                    f.write(event.to_json() + "\n")

            print(f"‚úÖ Exported {len(events)} log events to {output_file}")

    elif args.type == "benchmarks":
        # Export benchmark results
        harness = BenchmarkHarness(
            results_dir=args.results_dir or "./benchmark_results"
        )
        results = await harness.load_results()

        output_file = args.output or "exported_benchmarks.json"
        with open(output_file, "w") as f:
            json.dump(
                [result.to_dict() for result in results], f, indent=2, default=str
            )

        print(f"‚úÖ Exported {len(results)} benchmark results to {output_file}")


def main():
    """Main CLI entry point."""
    parser = argparse.ArgumentParser(
        description="Gary-Zero Unified Logging, Monitoring & Benchmarking CLI"
    )
    subparsers = parser.add_subparsers(dest="command", help="Available commands")

    # Logs command
    logs_parser = subparsers.add_parser("logs", help="Query and display log events")
    logs_parser.add_argument("--event-type", help="Filter by event type")
    logs_parser.add_argument("--level", help="Filter by log level")
    logs_parser.add_argument("--user-id", help="Filter by user ID")
    logs_parser.add_argument("--agent-id", help="Filter by agent ID")
    logs_parser.add_argument("--since", help="Show events from last N hours", type=str)
    logs_parser.add_argument(
        "--limit", help="Maximum number of events", type=int, default=50
    )
    logs_parser.add_argument(
        "--verbose", "-v", help="Show detailed information", action="store_true"
    )

    # Stats command
    stats_parser = subparsers.add_parser("stats", help="Display logging statistics")

    # Timeline command
    timeline_parser = subparsers.add_parser(
        "timeline", help="Display execution timeline"
    )
    timeline_parser.add_argument("--user-id", help="Filter by user ID")
    timeline_parser.add_argument("--agent-id", help="Filter by agent ID")
    timeline_parser.add_argument(
        "--since", help="Show events from last N hours", type=str
    )

    # Benchmark command
    benchmark_parser = subparsers.add_parser("benchmark", help="Run benchmark suite")
    benchmark_parser.add_argument("--task-ids", help="Comma-separated task IDs to run")
    benchmark_parser.add_argument(
        "--configs", help="Comma-separated config names to use"
    )
    benchmark_parser.add_argument("--tasks-file", help="JSON file with custom tasks")
    benchmark_parser.add_argument("--output", help="Output directory for results")
    benchmark_parser.add_argument(
        "--sequential", help="Run tests sequentially", action="store_true"
    )
    benchmark_parser.add_argument(
        "--parallel-count", help="Max parallel tests", type=int, default=3
    )
    benchmark_parser.add_argument(
        "--report", help="Generate summary report", action="store_true"
    )

    # Analyze command
    analyze_parser = subparsers.add_parser("analyze", help="Analyze benchmark results")
    analyze_parser.add_argument(
        "--results-dir",
        help="Directory with benchmark results",
        default="./benchmark_results",
    )
    analyze_parser.add_argument(
        "--compare-configs", help="Compare configurations", action="store_true"
    )
    analyze_parser.add_argument(
        "--detect-regressions", help="Detect regressions", action="store_true"
    )
    analyze_parser.add_argument(
        "--baseline-period", help="Baseline period in hours for regression detection"
    )

    # Export command
    export_parser = subparsers.add_parser("export", help="Export data")
    export_parser.add_argument(
        "type", choices=["logs", "benchmarks"], help="Type of data to export"
    )
    export_parser.add_argument("--output", help="Output file path")
    export_parser.add_argument(
        "--limit", help="Maximum number of records", type=int, default=1000
    )
    export_parser.add_argument(
        "--storage",
        choices=["memory", "sqlite"],
        default="memory",
        help="Storage backend for logs",
    )
    export_parser.add_argument("--db-path", help="SQLite database path")
    export_parser.add_argument(
        "--results-dir",
        help="Benchmark results directory",
        default="./benchmark_results",
    )

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        return

    # Map commands to functions
    commands = {
        "logs": cmd_logs,
        "stats": cmd_stats,
        "timeline": cmd_timeline,
        "benchmark": cmd_benchmark,
        "analyze": cmd_analyze,
        "export": cmd_export,
    }

    if args.command in commands:
        try:
            asyncio.run(commands[args.command](args))
        except KeyboardInterrupt:
            print("\nüõë Interrupted by user")
        except Exception as e:
            print(f"‚ùå Error: {e}")
            if "--verbose" in sys.argv or "-v" in sys.argv:
                import traceback

                traceback.print_exc()
    else:
        print(f"‚ùå Unknown command: {args.command}")
        parser.print_help()


if __name__ == "__main__":
    main()
